1*** explanation
2*** information


1*** explanation

[1] how does chat gpt work ?
[2] The transformer algorithm
[3] the steps of creating your own Chatbot GPT
[4] how many kind of neural network ?
[5] what is neural network ?
[6] why ChatGPT is only available as a cloud-based service ?
[7] how large is ChatGPT ?
[8] why The transformer algorithm is expensive ?
[9] what is The attention mechanism ?
[10] is it possible to build ChatGPT by diy ?
[11] is it possible to make it offline and self learning by it's own ?
[12] how big is it, the data server of chat gpt ?
[13] what is natural language processing ?
[14] recurrent connection & attention mechanism
[15] Perceptron neural network (bard explanation)
[16] Perceptron neural network (anakin explanation)
[17] what is Fine-tuning in A.I field ?
[18] what is pre-trained model ?
[19] what is overfitting ?
[20] let's take chat gpt as the example. how long does it take to pre-training the chat bot before fine-tunning it
[21] evaluation and iteration
[22] various metrics like response coherence, relevancy, fluency
[23] NAND gate
[24] Feedforward neural network
[25] application that use Feedforward neural network
[26] what is feedback loops ?
[27] feedback loops of Neural learning 
[28] does feedback loop apply on perceptron neural network ?
[29] what kind of transformer model architesture that RNNs is using ?
[30] FNNs (feedforward neural networks) can use transformer model architectures
[31] state-of-the-art transformer models
[32] supervised learning 
[33] regularization, early stopping, and cross-validation to prevent overfitting in machine learning
[34] the hyperparameters of the neural network, such as the learning rate and the number of epochs
[35] different between parameter and arguement ?
[36] long-term dependency 
[37] transformer model architecture
[38] long short-term memory (LSTM) networks
[39] language model 
[40] normalizing the output of an a.i chatbot. what does that mean ? 
[41] different between self attention & encoder-decoder attention ?
[42] Combining self-attention and encoder-decoder attention
[43] can feedback loops be viewed as a problem or just an aspect in neural learning ?
[44] AND logic gate ?
[45] different between AND and NAND ?
[46] Not logic gate ?
[47] OR logic gate ?
[48] recurrent neural network
[49] what is CNNs neural network ?
[50] Combining different types of neural networks and machine learning techniques in a single project
[51] BERT 
[52] Word2Vec or GloVe

2*** information

[1] replica A.I chatbot
[2] C.ai beta
[3] What kind of neural network does bard Google use ?
[4] generative adversarial network (GAN)
[5] Capcom used AI to create the hardest Street Fighter boss
[6] what kind of neural architecture does geminy a.i use ?
[7] Google Translate does indeed use a combination of self-attention and encoder-decoder attention
[8] Does spotify use neural network archictecture ?
[9] RNNs is a traditional neural network archicture 
[10] cloudflare sever
[11] does arduino or raspberry pi use NAND or AND ?
[12] so you mean RNN can create for the a.i an temporal memory (short term memory) how about long term memory ?
[13] Not all pre-trained models can be fine-tuned





